{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uplift Modeling for Churn Prediction\n",
    "\n",
    "Short notebook: same workflow as the full notebook, using **utils** for all logic.\n",
    "Run cells in order. Data paths: `train/` and `test/` under project root.\n",
    "\n",
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "try:\n",
    "    from causalml.metrics import qini_auc_score\n",
    "except Exception:\n",
    "    qini_auc_score = None\n",
    "\n",
    "from utils import (\n",
    "    print_table_overview, count_events_before_signup, time_bin, compute_uplift, plot_uplift_bars,\n",
    "    build_recency_tenure, load_wellco_brief, ref_date_from_tables, embed_wellco_brief, embed_visit_texts,\n",
    "    filter_wellco_relevant_visits, agg_web_features, agg_app_features, agg_claims_features,\n",
    "    agg_lifecycle_tenure, build_feature_matrix, make_lgbm, make_xgb,\n",
    "    uplift_at_k, uplift_curve, approx_auuc, assign_segments, _build_model,\n",
    "    DOW_NAMES, SIMILARITY_THRESHOLD, EMBED_MODEL_NAME, FOCUS_ICD_CODES, RANDOM_STATE,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "BASE_DIR = Path('.').resolve()\n",
    "TRAIN_DIR = BASE_DIR / 'train'\n",
    "TEST_DIR = BASE_DIR / 'test'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "Train/test CSVs; train events restricted to observation window (July 1\u201315, 2025).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_labels = pd.read_csv(TRAIN_DIR / 'churn_labels.csv', parse_dates=['signup_date'])\n",
    "app_usage = pd.read_csv(TRAIN_DIR / 'app_usage.csv', parse_dates=['timestamp'])\n",
    "web_visits = pd.read_csv(TRAIN_DIR / 'web_visits.csv', parse_dates=['timestamp'])\n",
    "claims = pd.read_csv(TRAIN_DIR / 'claims.csv', parse_dates=['diagnosis_date'])\n",
    "test_members = pd.read_csv(TEST_DIR / 'test_members.csv', parse_dates=['signup_date'])\n",
    "test_app_usage = pd.read_csv(TEST_DIR / 'test_app_usage.csv', parse_dates=['timestamp'])\n",
    "test_web_visits = pd.read_csv(TEST_DIR / 'test_web_visits.csv', parse_dates=['timestamp'])\n",
    "test_claims = pd.read_csv(TEST_DIR / 'test_claims.csv', parse_dates=['diagnosis_date'])\n",
    "\n",
    "OBS_START, OBS_END = pd.Timestamp('2025-07-01'), pd.Timestamp('2025-07-15')\n",
    "web_visits = web_visits[(web_visits['timestamp'] >= OBS_START) & (web_visits['timestamp'] < OBS_END)]\n",
    "app_usage  = app_usage[(app_usage['timestamp'] >= OBS_START) & (app_usage['timestamp'] < OBS_END)]\n",
    "claims     = claims[(claims['diagnosis_date'] >= OBS_START) & (claims['diagnosis_date'] < OBS_END)]\n",
    "\n",
    "for name, df in [('churn_labels', churn_labels), ('app_usage', app_usage), ('web_visits', web_visits),\n",
    "                  ('claims', claims), ('test_members', test_members), ('test_app_usage', test_app_usage),\n",
    "                  ('test_web_visits', test_web_visits), ('test_claims', test_claims)]:\n",
    "    print(f'{name}: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA\n",
    "### 3.1 Raw data overview\n",
    "Structure, dtypes, and sample for all 8 tables (utils: `print_table_overview`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = {'churn_labels': churn_labels, 'app_usage': app_usage, 'web_visits': web_visits, 'claims': claims,\n",
    "              'test_members': test_members, 'test_app_usage': test_app_usage, 'test_web_visits': test_web_visits, 'test_claims': test_claims}\n",
    "for name, df in all_tables.items():\n",
    "    print_table_overview(name, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Column-specific checks\n",
    "event_type, url, title, icd_code for feature-engineering decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('app_usage event_type:', app_usage['event_type'].value_counts().to_string())\n",
    "print('web_visits url unique:', web_visits['url'].nunique(), '| title unique:', web_visits['title'].nunique())\n",
    "print('claims icd_code unique:', claims['icd_code'].nunique())\n",
    "print(claims['icd_code'].value_counts().head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Missing values and member coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = []\n",
    "for name, df in all_tables.items():\n",
    "    for col, cnt in df.isnull().sum().items():\n",
    "        if cnt > 0: null_rows.append({'table': name, 'column': col, 'null_count': cnt})\n",
    "print('Column nulls:', pd.DataFrame(null_rows).to_string(index=False) if null_rows else 'None')\n",
    "base_ids = set(churn_labels['member_id'])\n",
    "n_base = len(base_ids)\n",
    "for src_name, src_df in [('web_visits', web_visits), ('app_usage', app_usage), ('claims', claims)]:\n",
    "    present = set(src_df['member_id']) & base_ids\n",
    "    print(f'{src_name}: {len(present)} present, {n_base - len(present)} absent ({100*(n_base-len(present))/n_base:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Missingness mechanism (Chi-square) and 3.5 Labels & treatment balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = churn_labels[['member_id', 'churn', 'outreach']].copy()\n",
    "train_ids['has_web'] = train_ids['member_id'].isin(web_visits['member_id']).astype(int)\n",
    "train_ids['has_app'] = train_ids['member_id'].isin(app_usage['member_id']).astype(int)\n",
    "train_ids['has_claims'] = train_ids['member_id'].isin(claims['member_id']).astype(int)\n",
    "for source in ['has_web', 'has_app', 'has_claims']:\n",
    "    for target in ['churn', 'outreach']:\n",
    "        ct = pd.crosstab(train_ids[source], train_ids[target])\n",
    "        chi2, p, _, _ = chi2_contingency(ct)\n",
    "        print(f'{source} vs {target}: chi2={chi2:.2f} p={p:.4g}')\n",
    "print('Churn rate:', churn_labels['churn'].mean(), '| Outreach rate:', churn_labels['outreach'].mean())\n",
    "print(churn_labels.groupby('outreach')['churn'].agg(['count', 'mean']).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Leakage & time-window validation (utils: `count_events_before_signup`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_summary = pd.DataFrame([\n",
    "    {'table': 'web_visits', 'min': web_visits['timestamp'].min(), 'max': web_visits['timestamp'].max()},\n",
    "    {'table': 'app_usage', 'min': app_usage['timestamp'].min(), 'max': app_usage['timestamp'].max()},\n",
    "    {'table': 'claims', 'min': claims['diagnosis_date'].min(), 'max': claims['diagnosis_date'].max()},\n",
    "])\n",
    "leakage = pd.DataFrame([\n",
    "    {'table': 'web_visits', 'events_before_signup': count_events_before_signup(web_visits, 'timestamp', churn_labels)},\n",
    "    {'table': 'app_usage', 'events_before_signup': count_events_before_signup(app_usage, 'timestamp', churn_labels)},\n",
    "    {'table': 'claims', 'events_before_signup': count_events_before_signup(claims, 'diagnosis_date', churn_labels)},\n",
    "])\n",
    "display(window_summary)\n",
    "display(leakage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Temporal & uplift helpers\n",
    "Prepare events; define `labels` for uplift; utils: `time_bin`, `compute_uplift`, `plot_uplift_bars`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_ev = web_visits[['member_id', 'timestamp']].copy(); web_ev['hour'] = web_ev['timestamp'].dt.hour; web_ev['dow'] = web_ev['timestamp'].dt.dayofweek\n",
    "app_ev = app_usage[['member_id', 'timestamp']].copy(); app_ev['hour'] = app_ev['timestamp'].dt.hour; app_ev['dow'] = app_ev['timestamp'].dt.dayofweek\n",
    "events = pd.concat([web_ev[['member_id', 'hour', 'dow']], app_ev[['member_id', 'hour', 'dow']]], ignore_index=True)\n",
    "events['time_of_day'] = events['hour'].apply(time_bin)\n",
    "events['dow_name'] = events['dow'].map(DOW_NAMES)\n",
    "events['is_weekend'] = events['dow'].isin([5, 6])\n",
    "labels = churn_labels[['member_id', 'churn', 'outreach']]\n",
    "print('Events:', len(events), 'rows,', events['member_id'].nunique(), 'members')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Uplift by time of day and day of week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_order = ['Early Morning', 'Morning', 'Afternoon', 'Evening']\n",
    "tod_uplift = [compute_uplift(labels, events.loc[events['time_of_day'] == t, 'member_id'].unique())[0] for t in tod_order]\n",
    "plot_uplift_bars(tod_order, tod_uplift, 'Uplift by time of day', 'Time of day')\n",
    "dow_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_uplift = [compute_uplift(labels, events.loc[events['dow_name'] == d, 'member_id'].unique())[0] for d in dow_order]\n",
    "plot_uplift_bars(dow_order, dow_uplift, 'Uplift by day of week', 'Day of week')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Recency & tenure (utils: `build_recency_tenure`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_df, ref_date = build_recency_tenure(churn_labels, web_visits, app_usage, claims)\n",
    "rec = churn_labels[['member_id', 'churn', 'outreach']].merge(recency_df, left_on='member_id', right_index=True)\n",
    "print('Ref date:', ref_date)\n",
    "print(rec[['days_since_last_web', 'days_since_last_app', 'days_since_last_activity', 'tenure_days']].describe().round(1).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Config; load WellCo brief and embedding model once; then build train/test feature matrices (utils).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WELLCO_BRIEF_PATH = BASE_DIR / 'wellco_client_brief.txt'\n",
    "brief_text = load_wellco_brief(WELLCO_BRIEF_PATH)\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "wellco_embedding = embed_wellco_brief(brief_text, embed_model)\n",
    "print('WellCo brief chars:', len(brief_text), '| Embedding shape:', wellco_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date_train = ref_date_from_tables(web_visits, app_usage, claims)\n",
    "ref_date_test  = ref_date_from_tables(test_web_visits, test_app_usage, test_claims)\n",
    "print('ref_date_train:', ref_date_train, '| ref_date_test:', ref_date_test)\n",
    "\n",
    "print('Building TRAIN feature matrix...')\n",
    "train_features = build_feature_matrix(churn_labels, web_visits, app_usage, claims, ref_date_train,\n",
    "    wellco_embedding=wellco_embedding, embed_model=embed_model, include_labels=True)\n",
    "print('Building TEST feature matrix...')\n",
    "test_features = build_feature_matrix(test_members, test_web_visits, test_app_usage, test_claims, ref_date_test,\n",
    "    wellco_embedding=wellco_embedding, embed_model=embed_model, include_labels=False)\n",
    "print('Train shape:', train_features.shape, '| Test shape:', test_features.shape)\n",
    "print('Columns:', list(train_features.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection \u2014 Uplift CV\n",
    "Stratified K-fold CV; compare S/T/X-learner \u00d7 LGBM/XGB with AUUC, Qini, uplift@k (utils: `_build_model`, metric helpers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = ['wellco_web_visits_count', 'days_since_last_wellco_web', 'app_sessions_count',\n",
    "                'icd_distinct_count', 'has_focus_icd', 'days_since_last_claim', 'tenure_days']\n",
    "N_SPLITS, N_CURVE_POINTS = 5, 100\n",
    "CANDIDATE_DEFS = [('S+LGBM','S','LGBM'),('S+XGB','S','XGB'),('T+LGBM','T','LGBM'),('T+XGB','T','XGB'),('X+LGBM','X','LGBM'),('X+XGB','X','XGB')]\n",
    "\n",
    "X = train_features[FEATURE_COLS].copy()\n",
    "y = train_features['churn'].astype(int).values\n",
    "treatment = train_features['outreach'].astype(int).values\n",
    "stratify_col = 2 * treatment + y\n",
    "SCALE_POS_WEIGHT = (y == 0).sum() / max((y == 1).sum(), 1)\n",
    "print('X shape:', X.shape, '| Churn rate:', y.mean(), '| Treatment rate:', treatment.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_records, cv_curves, cv_segments = [], {}, {}\n",
    "for name, meta_key, base_key in CANDIDATE_DEFS:\n",
    "    cv_curves[name], cv_segments[name] = [], []\n",
    "    for fold_i, (tr_idx, va_idx) in enumerate(skf.split(X, stratify_col), start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        t_tr, t_va = treatment[tr_idx], treatment[va_idx]\n",
    "        spw = (y_tr == 0).sum() / max((y_tr == 1).sum(), 1)\n",
    "        model = _build_model(meta_key, base_key, spw)\n",
    "        model.fit(X_tr, t_tr, y_tr)\n",
    "        tau = np.asarray(model.predict(X_va)).reshape(-1)\n",
    "        ks, uvals = uplift_curve(y_va, t_va, tau, n_points=N_CURVE_POINTS)\n",
    "        auuc_val = approx_auuc(ks, uvals)\n",
    "        qini_val = float(qini_auc_score(y_va, tau, t_va)) if qini_auc_score else np.nan\n",
    "        u10, u20 = uplift_at_k(y_va, t_va, tau, 0.10), uplift_at_k(y_va, t_va, tau, 0.20)\n",
    "        seg = assign_segments(tau)\n",
    "        seg_share = pd.Series(seg).value_counts(normalize=True)\n",
    "        cv_records.append({'model': name, 'fold': fold_i, 'auuc': auuc_val, 'qini': qini_val, 'uplift@10%': u10, 'uplift@20%': u20,\n",
    "                          'persuadables_pct': seg_share.get('Persuadables', 0)})\n",
    "        cv_curves[name].append((ks, uvals))\n",
    "        cv_segments[name].append(seg_share)\n",
    "        print(f'[{name}] Fold {fold_i}: AUUC={auuc_val:+.5f} Qini={qini_val:+.5f} u@10%={u10:+.4f} u@20%={u20:+.4f}')\n",
    "print('CV complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Results summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv_records)\n",
    "summary = cv_df.groupby('model').agg(auuc_mean=('auuc','mean'), auuc_std=('auuc','std'),\n",
    "    qini_mean=('qini','mean'), u10_mean=('uplift@10%','mean'), u20_mean=('uplift@20%','mean')).sort_values('auuc_mean', ascending=False)\n",
    "print('CV results (mean \u00b1 std):')\n",
    "display(summary.round(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Diagnostic plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "models = summary.index.tolist()\n",
    "x = np.arange(len(models))\n",
    "ax1.bar(x, summary['auuc_mean'], yerr=summary['auuc_std'], capsize=5); ax1.set_xticks(x); ax1.set_xticklabels(models, rotation=30, ha='right')\n",
    "ax1.set_ylabel('AUUC'); ax1.set_title('AUUC by model'); ax1.axhline(0, color='grey', ls='--')\n",
    "qini_std = cv_df.groupby('model')['qini'].std().reindex(models).fillna(0).values\n",
    "ax2.bar(x, summary['qini_mean'], yerr=qini_std, capsize=5); ax2.set_xticks(x); ax2.set_xticklabels(models, rotation=30, ha='right')\n",
    "ax2.set_ylabel('Qini'); ax2.set_title('Qini by model'); ax2.axhline(0, color='grey', ls='--')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}